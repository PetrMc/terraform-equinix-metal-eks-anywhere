#!/usr/bin/env bash
export TINKERBELL_HOST_IP=${tink_vip}
export CLUSTER_NAME=${cluster_name}
export TINKERBELL_PROVIDER=true
export API_TOKEN=${api_token}
declare -A NODES_ID
CREATE_CLUSTER_TIMEOUT="60m"
%{ for k, v in nodes_id ~}
NODES_ID["${k}"]="${v}"
%{ endfor ~}
CONTROL_PLANE_VIP=${pool_vip}
CLUSTER_CONFIG_FILE=$CLUSTER_NAME.yaml
PUB_SSH_KEY="$(cat /root/.ssh/${ssh_key_name}.pub)"
chmod 400 /root/.ssh/${ssh_key_name}.pub
chmod 400 /root/.ssh/${ssh_key_name}
# build cluster config file
eksctl anywhere generate clusterconfig $CLUSTER_NAME --provider tinkerbell > $CLUSTER_CONFIG_FILE
cp $CLUSTER_CONFIG_FILE $CLUSTER_CONFIG_FILE.orig
yq e -i "select(.kind == \"Cluster\").spec.controlPlaneConfiguration.endpoint.host |= \"$CONTROL_PLANE_VIP\"" $CLUSTER_CONFIG_FILE
yq e -i 'select(.kind == "Cluster").spec.controlPlaneConfiguration.count |= ${cp_device_count}' $CLUSTER_CONFIG_FILE
yq e -i 'select(.spec.workerNodeGroupConfigurations[].machineGroupRef.kind == "TinkerbellMachineConfig").spec.workerNodeGroupConfigurations[0].count |= ${dp_device_count}' $CLUSTER_CONFIG_FILE
yq e -i "select(.kind == \"TinkerbellDatacenterConfig\").spec.tinkerbellIP |= \"$TINKERBELL_HOST_IP\"" $CLUSTER_CONFIG_FILE
yq e -i "select(.kind == \"TinkerbellMachineConfig\").spec.users[].sshAuthorizedKeys[0] |= \"$PUB_SSH_KEY\"" $CLUSTER_CONFIG_FILE
yq e -i 'select(.kind == "TinkerbellMachineConfig").spec.osFamily |= "${node_device_os}"' $CLUSTER_CONFIG_FILE
yq e -i 'select(.kind == "TinkerbellMachineConfig").spec.hardwareSelector |= { "type": "HW_TYPE" }' $CLUSTER_CONFIG_FILE
yq e -i 'select(.kind == "TinkerbellMachineConfig").spec.templateRef.kind = "TinkerbellTemplateConfig"' $CLUSTER_CONFIG_FILE
yq e -i 'select(.kind == "TinkerbellMachineConfig").spec.templateRef.name = "${cluster_name}"' $CLUSTER_CONFIG_FILE
sed -i '0,/^\([[:blank:]]*\)type: HW_TYPE.*$/ s//\1type: cp/' $CLUSTER_CONFIG_FILE
sed -i '0,/^\([[:blank:]]*\)type: HW_TYPE.*$/ s//\1type: dp/' $CLUSTER_CONFIG_FILE
cat /root/tinkerbelltemplateconfig.yaml >> $CLUSTER_CONFIG_FILE
# execute reboot nodes script in background. It will check the log file generated by the `eksctl anywhere create cluster` command below
# and reboot the nodes once the text "Creating new workload cluster" is printed
chmod +x /root/reboot_nodes.sh
/root/reboot_nodes.sh "$(declare -p NODES_ID)" 2>&1 | tee -a /root/reboot_nodes.log &
# create eks cluster
timeout $CREATE_CLUSTER_TIMEOUT eksctl anywhere create cluster --filename $CLUSTER_CONFIG_FILE --hardware-csv hardware.csv --tinkerbell-bootstrap-ip ${pool_admin} 2>&1 | tee -a /root/eksa-create-cluster.log
